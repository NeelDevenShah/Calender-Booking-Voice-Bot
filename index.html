<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Assistant</title>
    <style>
      :root {
        --primary: #4a86e8;
        --secondary: #f0f0f5;
        --accent: #ff6b6b;
        --text: #333333;
        --light-text: #666666;
        --success: #4caf50;
        --danger: #f44336;
        --warning: #ff9800;
      }

      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
      }

      body {
        background-color: var(--secondary);
        color: var(--text);
        min-height: 100vh;
        display: flex;
        flex-direction: column;
      }

      .container {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        width: 100%;
        flex: 1;
        display: flex;
        flex-direction: column;
      }

      header {
        text-align: center;
        margin-bottom: 20px;
        padding-bottom: 15px;
        border-bottom: 1px solid #ddd;
      }

      h1 {
        color: var(--primary);
        margin-bottom: 10px;
      }

      .status-container {
        display: flex;
        align-items: center;
        justify-content: center;
        margin-bottom: 10px;
      }

      .status-label {
        font-weight: bold;
        margin-right: 10px;
      }

      .status {
        color: var(--primary);
        font-weight: 500;
      }

      .conversation-container {
        background-color: white;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        padding: 15px;
        margin-bottom: 20px;
        flex: 1;
        overflow-y: auto;
        max-height: 300px;
      }

      .message {
        margin-bottom: 15px;
        padding: 10px 15px;
        border-radius: 18px;
        max-width: 80%;
      }

      .user-message {
        background-color: var(--primary);
        color: white;
        align-self: flex-end;
        margin-left: auto;
      }

      .assistant-message {
        background-color: #e6e6e6;
        color: var(--text);
        align-self: flex-start;
      }

      .conversation-display {
        display: flex;
        flex-direction: column;
      }

      .visualization-container {
        margin: 20px 0;
        text-align: center;
      }

      .audio-level {
        width: 100%;
        height: 60px;
        background-color: #fff;
        border-radius: 5px;
        position: relative;
        overflow: hidden;
        box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
      }

      .level-bar {
        position: absolute;
        bottom: 0;
        left: 0;
        height: 100%;
        width: 0%;
        background-color: var(--primary);
        transition: width 0.1s ease;
      }

      .mic-container {
        display: flex;
        justify-content: center;
        align-items: center;
        margin: 20px 0;
      }

      .mic-button {
        width: 80px;
        height: 80px;
        border-radius: 50%;
        display: flex;
        justify-content: center;
        align-items: center;
        cursor: pointer;
        position: relative;
        background-color: var(--primary);
        box-shadow: 0 3px 10px rgba(74, 134, 232, 0.3);
        transition: all 0.2s ease;
      }

      .mic-button:hover {
        transform: scale(1.05);
        box-shadow: 0 5px 15px rgba(74, 134, 232, 0.4);
      }

      .mic-button.recording {
        background-color: var(--danger);
        box-shadow: 0 3px 10px rgba(244, 67, 54, 0.3);
        animation: pulse 1.5s infinite;
      }

      .mic-icon {
        color: white;
        font-size: 36px;
      }

      .controls {
        display: flex;
        justify-content: center;
        gap: 20px;
        margin: 10px 0;
      }

      .button {
        padding: 10px 20px;
        border: none;
        border-radius: 5px;
        background-color: var(--primary);
        color: white;
        font-weight: bold;
        cursor: pointer;
        transition: all 0.2s ease;
      }

      .button:hover {
        background-color: var(--accent);
        transform: translateY(-2px);
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
      }

      .button:disabled {
        background-color: #cccccc;
        cursor: not-allowed;
      }

      .pulse-ring {
        position: absolute;
        width: 100%;
        height: 100%;
        border-radius: 50%;
        opacity: 0;
        border: 3px solid var(--accent);
        animation: pulse-ring 1.5s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
      }

      @keyframes pulse-ring {
        0% {
          transform: scale(0.95);
          opacity: 0.5;
        }
        80%,
        100% {
          transform: scale(1.3);
          opacity: 0;
        }
      }

      @keyframes pulse {
        0% {
          transform: scale(1);
        }
        50% {
          transform: scale(1.05);
        }
        100% {
          transform: scale(1);
        }
      }

      .footer {
        text-align: center;
        margin-top: auto;
        padding: 10px;
        color: var(--light-text);
        font-size: 0.9em;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <header>
        <h1>Voice Assistant</h1>
        <div class="status-container">
          <div class="status-label">Status:</div>
          <div id="status" class="status">Ready</div>
        </div>
      </header>

      <div class="conversation-container">
        <div id="conversation" class="conversation-display">
          <div class="message assistant-message">
            Hello! How can I help you today?
          </div>
        </div>
      </div>

      <div class="visualization-container">
        <div class="audio-level">
          <div id="level-bar" class="level-bar"></div>
        </div>
      </div>

      <div class="mic-container">
        <div id="mic-button" class="mic-button">
          <span class="mic-icon">ðŸŽ¤</span>
        </div>
      </div>

      <div class="controls">
        <button id="calibrate-button" class="button">
          Calibrate Microphone
        </button>
      </div>

      <div class="footer">
        <p>Â© 2025 Voice Assistant</p>
      </div>
    </div>

    <script>
      // Audio settings
      const SAMPLE_RATE = 44100;
      const CHANNELS = 1;
      let SILENCE_THRESHOLD = 0.01;
      let SILENCE_DURATION = 0.4; // seconds
      let MIN_SPEECH_THRESHOLD = 0.02;
      let MIN_AUDIO_DURATION = 0.15; // seconds
      const DEBUG = true; // Enable debug logging

      // UI Elements
      const micButton = document.getElementById("mic-button");
      const levelBar = document.getElementById("level-bar");
      const statusElement = document.getElementById("status");
      const calibrateButton = document.getElementById("calibrate-button");
      const conversationElement = document.getElementById("conversation");

      // State variables
      let isRecording = false;
      let audioContext = null;
      let mediaStream = null;
      let mediaRecorder = null;
      let audioChunks = [];
      let silenceStart = null;
      let audioStartTime = null;
      let hasVoiceActivity = false;
      let analyserNode = null;
      let audioSource = null;

      // Debug logging function
      function debugLog(message) {
        if (DEBUG) {
          console.log(`[DEBUG] ${message}`);
        }
      }

      // Function to clear conversation history on page load
      async function clearConversationHistory() {
        try {
          const response = await fetch(
            "http://127.0.0.1:5001/api/clear-history",
            {
              method: "GET",
            }
          );

          const data = await response.json();
          if (data.success) {
            console.log("Conversation history cleared:", data.message);
          } else {
            console.error("Failed to clear history:", data.message);
          }
        } catch (error) {
          console.error("Error clearing conversation history:", error);
        }
      }

      // Call the function when the page loads
      window.addEventListener("load", clearConversationHistory);

      // Initialize functionality
      async function init() {
        micButton.addEventListener("click", toggleRecording);
        calibrateButton.addEventListener("click", calibrateMicrophone);

        try {
          // Pre-request access to microphone
          audioContext = new (window.AudioContext || window.webkitAudioContext)(
            { sampleRate: SAMPLE_RATE }
          );

          // Just initialize the audio context but don't request media stream yet
          updateStatus("Ready to use");
        } catch (err) {
          console.error("Error initializing audio:", err);
          updateStatus("Microphone access denied");
          micButton.disabled = true;
          calibrateButton.disabled = true;
        }
      }

      // Update UI status
      function updateStatus(message) {
        statusElement.textContent = message;
        debugLog(`Status: ${message}`);
      }

      // Handle microphone button click
      async function toggleRecording() {
        if (!isRecording) {
          startRecording();
        } else {
          stopRecording();
        }
      }

      // Start recording
      async function startRecording() {
        if (!audioContext || audioContext.state === "closed") {
          audioContext = new (window.AudioContext || window.webkitAudioContext)(
            { sampleRate: SAMPLE_RATE }
          );
        }

        if (audioContext.state === "suspended") {
          await audioContext.resume();
        }

        try {
          debugLog("Requesting microphone access...");
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });
          debugLog("Microphone access granted");

          // Set up audio analyzer
          analyserNode = audioContext.createAnalyser();
          analyserNode.fftSize = 2048; // More detailed FFT
          analyserNode.smoothingTimeConstant = 0.8; // Smoother transitions

          audioSource = audioContext.createMediaStreamSource(mediaStream);
          audioSource.connect(analyserNode);

          debugLog("Audio analyzer connected");

          // Testing with a tone if needed for debugging
          // const oscillator = audioContext.createOscillator();
          // oscillator.type = 'sine';
          // oscillator.frequency.setValueAtTime(440, audioContext.currentTime);
          // oscillator.connect(analyserNode);
          // oscillator.start();
          // setTimeout(() => oscillator.stop(), 1000);

          // Setup media recorder with specific options
          const options = { mimeType: "audio/webm" };
          try {
            mediaRecorder = new MediaRecorder(mediaStream, options);
            debugLog("MediaRecorder created with mime type: audio/webm");
          } catch (e) {
            debugLog("Failed with audio/webm, trying default mime type");
            mediaRecorder = new MediaRecorder(mediaStream);
          }

          audioChunks = [];

          // Start recording
          mediaRecorder.start(100); // Request data every 100ms
          isRecording = true;
          updateStatus("Listening...");
          micButton.classList.add("recording");
          micButton.innerHTML =
            '<div class="pulse-ring"></div><span class="mic-icon">ðŸŽ¤</span>';
          calibrateButton.disabled = true;

          // Create audio level meter
          audioStartTime = Date.now();
          silenceStart = null;
          hasVoiceActivity = false;

          // Monitor audio levels
          const bufferLength = analyserNode.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);

          const updateAudioLevel = () => {
            if (!isRecording) return;

            analyserNode.getByteFrequencyData(dataArray);

            // Calculate average level from frequency data
            let sum = 0;
            let count = 0;
            for (let i = 0; i < bufferLength; i++) {
              if (i > 5) {
                // Skip the lowest frequencies (often noise)
                sum += dataArray[i];
                count++;
              }
            }
            const average = count > 0 ? sum / count / 255 : 0; // Normalize to 0-1

            // Log audio levels periodically
            if (Date.now() % 1000 < 100) {
              debugLog(`Current audio level: ${average.toFixed(4)}`);
            }

            // Update level bar
            levelBar.style.width = `${average * 100}%`;

            // Change color based on level
            if (average < 0.3) {
              levelBar.style.backgroundColor = "var(--primary)";
            } else if (average < 0.6) {
              levelBar.style.backgroundColor = "var(--warning)";
            } else {
              levelBar.style.backgroundColor = "var(--danger)";
            }

            // Check for voice activity with adjusted thresholds
            if (average > MIN_SPEECH_THRESHOLD) {
              if (!hasVoiceActivity) {
                debugLog(`Voice activity detected: ${average.toFixed(4)}`);
                hasVoiceActivity = true;
              }
              silenceStart = null;
            } else if (average < SILENCE_THRESHOLD) {
              if (silenceStart === null) {
                silenceStart = Date.now();
                debugLog(`Silence started at ${silenceStart}`);
              } else if (
                hasVoiceActivity &&
                Date.now() - silenceStart > SILENCE_DURATION * 1000
              ) {
                // Stop recording after silence if we had voice activity
                debugLog(
                  `Stopping due to silence: ${Date.now() - silenceStart}ms`
                );
                stopRecording();
                return;
              }
            }

            // Auto-stop after 10 seconds maximum
            if (Date.now() - audioStartTime > 10000) {
              debugLog("Auto-stopping after 10 seconds");
              stopRecording();
              return;
            }

            requestAnimationFrame(updateAudioLevel);
          };

          requestAnimationFrame(updateAudioLevel);

          // Handle data chunks
          mediaRecorder.ondataavailable = (event) => {
            if (event.data && event.data.size > 0) {
              debugLog(`Received audio chunk: ${event.data.size} bytes`);
              audioChunks.push(event.data);
            }
          };

          // Handle recording stop
          mediaRecorder.onstop = async () => {
            debugLog(
              `Recording stopped. Has voice activity: ${hasVoiceActivity}`
            );
            debugLog(
              `Audio duration: ${(Date.now() - audioStartTime) / 1000}s`
            );
            debugLog(`Audio chunks: ${audioChunks.length}`);

            const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
            debugLog(`Audio blob size: ${audioBlob.size} bytes`);

            // For debugging - allow playback of the recorded audio
            const audioUrl = URL.createObjectURL(audioBlob);
            debugLog(`Audio URL: ${audioUrl}`);

            // Force process audio for testing purposes
            if (audioChunks.length > 0) {
              processAudio(audioBlob);
            } else {
              updateStatus("No audio data captured");
              setTimeout(() => updateStatus("Ready"), 2000);
            }
          };
        } catch (err) {
          console.error("Error starting recording:", err);
          updateStatus("Error starting recording");
          resetUI();
        }
      }

      // Stop recording
      function stopRecording() {
        if (mediaRecorder && isRecording) {
          try {
            debugLog("Stopping media recorder");
            isRecording = false;
            micButton.classList.remove("recording");
            micButton.innerHTML = '<span class="mic-icon">ðŸŽ¤</span>';
            updateStatus("Processing...");

            // Stop the MediaRecorder first
            mediaRecorder.stop();

            // Then stop all tracks on the stream
            if (mediaStream) {
              mediaStream.getTracks().forEach((track) => {
                track.stop();
                debugLog(`Stopped track: ${track.kind}`);
              });
            }

            // Disconnect and clean up audio nodes
            if (audioSource) {
              audioSource.disconnect();
              audioSource = null;
            }

            calibrateButton.disabled = false;
          } catch (err) {
            console.error("Error stopping recording:", err);
            updateStatus("Error occurred while stopping");
            resetUI();
          }
        }
      }

      // Process recorded audio
      async function processAudio(audioBlob) {
        const formData = new FormData();
        formData.append("file", audioBlob, "recording.wav");

        updateStatus("Sending to API...");

        try {
          const response = await fetch("http://127.0.0.1:5001/api/voice", {
            method: "POST",
            body: formData,
          });

          if (!response.ok) {
            throw new Error("Failed to process audio");
          }

          // Extract text messages from headers
          const userMessage = response.headers.get("X-Transcript");
          const assistantMessage = response.headers.get("X-Response");

          console.log("$$$$$$$$$$$$$$$$$$$$$$$$$$");
          console.log(userMessage);
          console.log(assistantMessage);
          console.log("$$$$$$$$$$$$$$$$$$$$$$$$$$");
          // Add messages to conversation
          if (userMessage) {
            addMessage(userMessage, "user");
          }

          if (assistantMessage) {
            addMessage(assistantMessage, "assistant");
          }

          // Convert the response to a blob (since it's an audio file)
          const audioBlob = await response.blob();

          // Play the generated speech audio
          playAudio(audioBlob);

          updateStatus("Ready");
        } catch (error) {
          console.error("Error processing audio:", error);
          updateStatus("Error processing audio");
        }
      }

      // Add message to conversation display
      function addMessage(text, sender) {
        const messageDiv = document.createElement("div");
        messageDiv.classList.add("message");
        messageDiv.classList.add(
          sender === "user" ? "user-message" : "assistant-message"
        );
        messageDiv.textContent = text;

        conversationElement.appendChild(messageDiv);
        conversationElement.scrollTop = conversationElement.scrollHeight;
        debugLog(`Added ${sender} message: ${text.substring(0, 30)}...`);
      }

      // Reset UI elements
      function resetUI() {
        isRecording = false;
        micButton.classList.remove("recording");
        micButton.innerHTML = '<span class="mic-icon">ðŸŽ¤</span>';
        levelBar.style.width = "0%";
        calibrateButton.disabled = false;
      }

      // Calibrate microphone
      async function calibrateMicrophone() {
        updateStatus("Calibrating microphone...");
        calibrateButton.disabled = true;
        micButton.disabled = true;

        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });

          if (!audioContext || audioContext.state === "closed") {
            audioContext = new (window.AudioContext ||
              window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });
          }

          if (audioContext.state === "suspended") {
            await audioContext.resume();
          }

          const source = audioContext.createMediaStreamSource(stream);
          const analyser = audioContext.createAnalyser();
          analyser.fftSize = 2048;
          analyser.smoothingTimeConstant = 0.8;
          source.connect(analyser);

          debugLog("Calibration audio analyzer connected");

          const bufferLength = analyser.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          let samples = [];

          // Collect samples for 3 seconds
          const startTime = Date.now();
          debugLog("Starting microphone calibration");

          const collectSamples = () => {
            if (Date.now() - startTime < 3000) {
              analyser.getByteFrequencyData(dataArray);

              let sum = 0;
              let count = 0;
              for (let i = 0; i < bufferLength; i++) {
                if (i > 5) {
                  // Skip the lowest frequencies
                  sum += dataArray[i];
                  count++;
                }
              }
              const average = count > 0 ? sum / count / 255 : 0;

              samples.push(average);

              // Log audio levels periodically during calibration
              if (Date.now() % 500 < 100) {
                debugLog(`Calibration level: ${average.toFixed(4)}`);
              }

              // Visualize calibration in progress
              levelBar.style.width = `${average * 100}%`;

              requestAnimationFrame(collectSamples);
            } else {
              finishCalibration(samples);

              // Stop all tracks on the stream
              stream.getTracks().forEach((track) => track.stop());
            }
          };

          requestAnimationFrame(collectSamples);
        } catch (err) {
          console.error("Error calibrating:", err);
          updateStatus("Error calibrating microphone");
          calibrateButton.disabled = false;
          micButton.disabled = false;
        }
      }

      // Process calibration data
      function finishCalibration(samples) {
        // Calculate noise statistics
        const sum = samples.reduce((a, b) => a + b, 0);
        const mean = sum / samples.length;

        // Calculate standard deviation
        const variance =
          samples.reduce((a, b) => a + Math.pow(b - mean, 2), 0) /
          samples.length;
        const stdDev = Math.sqrt(variance);

        // Set thresholds based on noise statistics
        // Adjust for more sensitivity if needed
        SILENCE_THRESHOLD = mean + stdDev * 0.5; // Less than before (was +stdDev)
        MIN_SPEECH_THRESHOLD = mean + stdDev * 2; // Less than before (was +stdDev*3)
        SILENCE_DURATION = 0.55; // Make silence detection a bit more patient

        // Ensure minimum thresholds in case of very quiet environments
        SILENCE_THRESHOLD = Math.max(SILENCE_THRESHOLD, 0.005);
        MIN_SPEECH_THRESHOLD = Math.max(MIN_SPEECH_THRESHOLD, 0.01);

        updateStatus("Calibration complete");
        levelBar.style.width = "0%";

        calibrateButton.disabled = false;
        micButton.disabled = false;

        debugLog(
          `Calibration results: Mean=${mean.toFixed(
            4
          )}, StdDev=${stdDev.toFixed(4)}`
        );
        debugLog(
          `New settings: SILENCE_THRESHOLD=${SILENCE_THRESHOLD.toFixed(
            4
          )}, MIN_SPEECH_THRESHOLD=${MIN_SPEECH_THRESHOLD.toFixed(
            4
          )}, SILENCE_DURATION=${SILENCE_DURATION}s`
        );
      }

      // Function to play the generated speech audio
      function playAudio(audioBlob) {
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        audio.play().catch((err) => console.error("Error playing audio:", err));
      }

      // Initialize when page loads
      window.addEventListener("load", init);
    </script>
  </body>
</html>
